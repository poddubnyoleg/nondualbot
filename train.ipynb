{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poddubnyoleg/nondualbot/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eGFSolPe6pD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events\n",
        "\n",
        "https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "4kwEF0k49Yv5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Nisargadatta_Maharaj.jpg/220px-Nisargadatta_Maharaj.jpg)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "“Wisdom tells me I am nothing, love tells me I am everything. Between the two, my life flows.” \n",
        "― Sri Nisargadatta Maharaj```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I got idea of this project while reading [famous article of Andrey Karpathy about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and thinking about if i can do something useful with generative ability of RNN. As I need to use neural nets in my [other project](https://github.com/poddubnyoleg/eeg_neurofeedback) with EEG data processing, it was fun to get NN experince in this fun project.\n",
        "\n",
        "\n",
        "Core idea of project is to create a chat bot, that can keep any discussion and provide user with some useful (sometimes :) knowledge. I pick Sri Nisargadatta Maharaj as a knowledge carrier, whom i love for beutiful non-dual poetry, and his most famous book 'I am that' as data source for training.\n",
        "\n",
        "In this notebook i guide you with the process of creating response engine, that can be based on your favourite text corpus."
      ]
    },
    {
      "metadata": {
        "id": "bGc0Vcfy2zQJ",
        "colab_type": "code",
        "outputId": "adc05ce1-d8aa-45e3-9236-977236f75340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "import sys\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fmtvsVFVEXIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load text data\n",
        "I use text file from google drive here. All preprocessing is considered to be done before"
      ]
    },
    {
      "metadata": {
        "id": "zxZbSUTb4lTk",
        "colab_type": "code",
        "outputId": "05f842e4-e64c-4d65-f9cc-f208728e8310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8JWl2n_n6LVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/My Drive/nondualbot/i_am_that.txt', 'r') \n",
        "data = file.read() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwJjADvv7vzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ea1d926-40b1-442e-87dd-c2f1859dce47"
      },
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That which permeates all, which nothing transcends and which, like the universal space around us, fi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "H_l_OgK1FjW7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "edNZraxrSXoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(data)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSO_sRT_Sq4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be1c8d5a-dcd8-4858-ea02-6cac3a105bdc"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(data)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  988168\n",
            "Total Vocab:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G6xQoIj3S2M7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each chunk of data [seq_length] we take one-hot-encoded dataX array and dataY array (same but shifted +1)"
      ]
    },
    {
      "metadata": {
        "id": "qdJx7TCHUniR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "fafeb5fe-a305-4457-df86-92facb55a9ce"
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(500, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(LSTM(500, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(LSTM(500, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, None, 500)         1112000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 500)         2002000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 500)         2002000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 55)          27555     \n",
            "=================================================================\n",
            "Total params: 5,143,555\n",
            "Trainable params: 5,143,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmkaoEyLZYc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "86aa1347-1506-4a7b-b787-d04ba859a719"
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-f257bfb4fac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 0 layers into a model with 4 layers."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "i9G0Bo1CZOYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7KEkkjrFOHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_data = [char_to_int[char] for char in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW5WyuvU-gUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**train with batch generator with fixed or variable batch sequense size**\n",
        "* Inner state doesnt share between samples inside of one batch, but can share between batches, if stateful=True\n",
        "* fit(shuffle=) doesn't relate to RNNs, because to keep inner state consistent withi epoch, you need to set stateful=True and batch_size=1, so there is nothing to shuffle\n",
        "* https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ubC4zNNSEOiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "def train_generator():\n",
        "  while True:\n",
        "\n",
        "    #seq_length = np.random.randint(low=2, high=200)\n",
        "    #x_train, y_train = [], []\n",
        "    \n",
        "    cur_pos = np.random.randint(low=0, high=n_chars-batch_size-1)\n",
        "\n",
        "    x_train = np_utils.to_categorical(num_data[cur_pos:cur_pos+batch_size], num_classes=n_vocab)\n",
        "    y_train = np_utils.to_categorical(num_data[cur_pos+1:cur_pos+batch_size+1], num_classes=n_vocab)\n",
        "\n",
        "    yield np.reshape(x_train, (1, batch_size, n_vocab)), np.reshape(y_train, (1, batch_size, n_vocab))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjtK8ogbGItq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9612
        },
        "outputId": "2ba3ca8b-15b8-4ca8-eea8-a0ca0f7b9a29"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator(), steps_per_epoch=100, epochs=200, verbose=1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "100/100 [==============================] - 49s 492ms/step - loss: 3.0976\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.09758, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 2.9366\n",
            "\n",
            "Epoch 00002: loss improved from 3.09758 to 2.93655, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 2.6773\n",
            "\n",
            "Epoch 00003: loss improved from 2.93655 to 2.67734, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 48s 475ms/step - loss: 2.3727\n",
            "\n",
            "Epoch 00004: loss improved from 2.67734 to 2.37265, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 2.2552\n",
            "\n",
            "Epoch 00005: loss improved from 2.37265 to 2.25517, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 2.1658\n",
            "\n",
            "Epoch 00006: loss improved from 2.25517 to 2.16583, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 2.0976\n",
            "\n",
            "Epoch 00007: loss improved from 2.16583 to 2.09760, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 2.0308\n",
            "\n",
            "Epoch 00008: loss improved from 2.09760 to 2.03078, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.9442\n",
            "\n",
            "Epoch 00009: loss improved from 2.03078 to 1.94422, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 1.8703\n",
            "\n",
            "Epoch 00010: loss improved from 1.94422 to 1.87028, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.8386\n",
            "\n",
            "Epoch 00011: loss improved from 1.87028 to 1.83862, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.8429\n",
            "\n",
            "Epoch 00012: loss did not improve from 1.83862\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.7699\n",
            "\n",
            "Epoch 00013: loss improved from 1.83862 to 1.76992, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.6813\n",
            "\n",
            "Epoch 00014: loss improved from 1.76992 to 1.68128, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 1.6701\n",
            "\n",
            "Epoch 00015: loss improved from 1.68128 to 1.67014, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.6305\n",
            "\n",
            "Epoch 00016: loss improved from 1.67014 to 1.63050, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 1.6032\n",
            "\n",
            "Epoch 00017: loss improved from 1.63050 to 1.60323, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.6155\n",
            "\n",
            "Epoch 00018: loss did not improve from 1.60323\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.5688\n",
            "\n",
            "Epoch 00019: loss improved from 1.60323 to 1.56877, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.5583\n",
            "\n",
            "Epoch 00020: loss improved from 1.56877 to 1.55833, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.4747\n",
            "\n",
            "Epoch 00021: loss improved from 1.55833 to 1.47467, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.5350\n",
            "\n",
            "Epoch 00022: loss did not improve from 1.47467\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.5169\n",
            "\n",
            "Epoch 00023: loss did not improve from 1.47467\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.4650\n",
            "\n",
            "Epoch 00024: loss improved from 1.47467 to 1.46499, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.4551\n",
            "\n",
            "Epoch 00025: loss improved from 1.46499 to 1.45515, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.4279\n",
            "\n",
            "Epoch 00026: loss improved from 1.45515 to 1.42792, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.3743\n",
            "\n",
            "Epoch 00027: loss improved from 1.42792 to 1.37433, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.4279\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.37433\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.3514\n",
            "\n",
            "Epoch 00029: loss improved from 1.37433 to 1.35141, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 1.4363\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.35141\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.4243\n",
            "\n",
            "Epoch 00031: loss did not improve from 1.35141\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 1.4182\n",
            "\n",
            "Epoch 00032: loss did not improve from 1.35141\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.3676\n",
            "\n",
            "Epoch 00033: loss did not improve from 1.35141\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.3873\n",
            "\n",
            "Epoch 00034: loss did not improve from 1.35141\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.3720\n",
            "\n",
            "Epoch 00035: loss did not improve from 1.35141\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.3507\n",
            "\n",
            "Epoch 00036: loss improved from 1.35141 to 1.35069, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.3736\n",
            "\n",
            "Epoch 00037: loss did not improve from 1.35069\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.3481\n",
            "\n",
            "Epoch 00038: loss improved from 1.35069 to 1.34812, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 46s 465ms/step - loss: 1.3050\n",
            "\n",
            "Epoch 00039: loss improved from 1.34812 to 1.30505, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.3400\n",
            "\n",
            "Epoch 00040: loss did not improve from 1.30505\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 1.2902\n",
            "\n",
            "Epoch 00041: loss improved from 1.30505 to 1.29024, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 1.3432\n",
            "\n",
            "Epoch 00042: loss did not improve from 1.29024\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 1.2909\n",
            "\n",
            "Epoch 00043: loss did not improve from 1.29024\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 1.3047\n",
            "\n",
            "Epoch 00044: loss did not improve from 1.29024\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 1.2723\n",
            "\n",
            "Epoch 00045: loss improved from 1.29024 to 1.27231, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 1.2749\n",
            "\n",
            "Epoch 00046: loss did not improve from 1.27231\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 1.2730\n",
            "\n",
            "Epoch 00047: loss did not improve from 1.27231\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 1.2802\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.27231\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 1.2733\n",
            "\n",
            "Epoch 00049: loss did not improve from 1.27231\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 1.2674\n",
            "\n",
            "Epoch 00050: loss improved from 1.27231 to 1.26737, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 1.2529\n",
            "\n",
            "Epoch 00051: loss improved from 1.26737 to 1.25290, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.2423\n",
            "\n",
            "Epoch 00052: loss improved from 1.25290 to 1.24233, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.2953\n",
            "\n",
            "Epoch 00053: loss did not improve from 1.24233\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 1.2667\n",
            "\n",
            "Epoch 00054: loss did not improve from 1.24233\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 1.2725\n",
            "\n",
            "Epoch 00055: loss did not improve from 1.24233\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 1.2468\n",
            "\n",
            "Epoch 00056: loss did not improve from 1.24233\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 1.2460\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.24233\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.2672\n",
            "\n",
            "Epoch 00058: loss did not improve from 1.24233\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.2281\n",
            "\n",
            "Epoch 00059: loss improved from 1.24233 to 1.22806, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.2495\n",
            "\n",
            "Epoch 00060: loss did not improve from 1.22806\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 46s 465ms/step - loss: 1.2472\n",
            "\n",
            "Epoch 00061: loss did not improve from 1.22806\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.2215\n",
            "\n",
            "Epoch 00062: loss improved from 1.22806 to 1.22155, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.2465\n",
            "\n",
            "Epoch 00063: loss did not improve from 1.22155\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 1.2579\n",
            "\n",
            "Epoch 00064: loss did not improve from 1.22155\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.1997\n",
            "\n",
            "Epoch 00065: loss improved from 1.22155 to 1.19971, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 46s 465ms/step - loss: 1.2308\n",
            "\n",
            "Epoch 00066: loss did not improve from 1.19971\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.2552\n",
            "\n",
            "Epoch 00067: loss did not improve from 1.19971\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.2082\n",
            "\n",
            "Epoch 00068: loss did not improve from 1.19971\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.2192\n",
            "\n",
            "Epoch 00069: loss did not improve from 1.19971\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.1772\n",
            "\n",
            "Epoch 00070: loss improved from 1.19971 to 1.17720, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 1.2291\n",
            "\n",
            "Epoch 00071: loss did not improve from 1.17720\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.2365\n",
            "\n",
            "Epoch 00072: loss did not improve from 1.17720\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.2088\n",
            "\n",
            "Epoch 00073: loss did not improve from 1.17720\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.1866\n",
            "\n",
            "Epoch 00074: loss did not improve from 1.17720\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.2277\n",
            "\n",
            "Epoch 00075: loss did not improve from 1.17720\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.2267\n",
            "\n",
            "Epoch 00076: loss did not improve from 1.17720\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 1.1948\n",
            "\n",
            "Epoch 00077: loss did not improve from 1.17720\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 1.1870\n",
            "\n",
            "Epoch 00078: loss did not improve from 1.17720\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.1869\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.17720\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1725\n",
            "\n",
            "Epoch 00080: loss improved from 1.17720 to 1.17251, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.2502\n",
            "\n",
            "Epoch 00081: loss did not improve from 1.17251\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1872\n",
            "\n",
            "Epoch 00082: loss did not improve from 1.17251\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1683\n",
            "\n",
            "Epoch 00083: loss improved from 1.17251 to 1.16832, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.1838\n",
            "\n",
            "Epoch 00084: loss did not improve from 1.16832\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1913\n",
            "\n",
            "Epoch 00085: loss did not improve from 1.16832\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1822\n",
            "\n",
            "Epoch 00086: loss did not improve from 1.16832\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1962\n",
            "\n",
            "Epoch 00087: loss did not improve from 1.16832\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1915\n",
            "\n",
            "Epoch 00088: loss did not improve from 1.16832\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 1.1747\n",
            "\n",
            "Epoch 00089: loss did not improve from 1.16832\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 49s 488ms/step - loss: 1.1574\n",
            "\n",
            "Epoch 00090: loss improved from 1.16832 to 1.15743, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 1.1637\n",
            "\n",
            "Epoch 00091: loss did not improve from 1.15743\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1762\n",
            "\n",
            "Epoch 00092: loss did not improve from 1.15743\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1629\n",
            "\n",
            "Epoch 00093: loss did not improve from 1.15743\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1837\n",
            "\n",
            "Epoch 00094: loss did not improve from 1.15743\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1826\n",
            "\n",
            "Epoch 00095: loss did not improve from 1.15743\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1523\n",
            "\n",
            "Epoch 00096: loss improved from 1.15743 to 1.15229, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.1685\n",
            "\n",
            "Epoch 00097: loss did not improve from 1.15229\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.1891\n",
            "\n",
            "Epoch 00098: loss did not improve from 1.15229\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1748\n",
            "\n",
            "Epoch 00099: loss did not improve from 1.15229\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.1703\n",
            "\n",
            "Epoch 00100: loss did not improve from 1.15229\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 1.1912\n",
            "\n",
            "Epoch 00101: loss did not improve from 1.15229\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 46s 465ms/step - loss: 1.1998\n",
            "\n",
            "Epoch 00102: loss did not improve from 1.15229\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.1639\n",
            "\n",
            "Epoch 00103: loss did not improve from 1.15229\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1517\n",
            "\n",
            "Epoch 00104: loss improved from 1.15229 to 1.15174, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1613\n",
            "\n",
            "Epoch 00105: loss did not improve from 1.15174\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.1320\n",
            "\n",
            "Epoch 00106: loss improved from 1.15174 to 1.13204, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1407\n",
            "\n",
            "Epoch 00107: loss did not improve from 1.13204\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1507\n",
            "\n",
            "Epoch 00108: loss did not improve from 1.13204\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1619\n",
            "\n",
            "Epoch 00109: loss did not improve from 1.13204\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.1581\n",
            "\n",
            "Epoch 00110: loss did not improve from 1.13204\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.1677\n",
            "\n",
            "Epoch 00111: loss did not improve from 1.13204\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1592\n",
            "\n",
            "Epoch 00112: loss did not improve from 1.13204\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 1.1102\n",
            "\n",
            "Epoch 00113: loss improved from 1.13204 to 1.11022, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 1.1342\n",
            "\n",
            "Epoch 00114: loss did not improve from 1.11022\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 1.1209\n",
            "\n",
            "Epoch 00115: loss did not improve from 1.11022\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1182\n",
            "\n",
            "Epoch 00116: loss did not improve from 1.11022\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1551\n",
            "\n",
            "Epoch 00117: loss did not improve from 1.11022\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1734\n",
            "\n",
            "Epoch 00118: loss did not improve from 1.11022\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1643\n",
            "\n",
            "Epoch 00119: loss did not improve from 1.11022\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1685\n",
            "\n",
            "Epoch 00120: loss did not improve from 1.11022\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1312\n",
            "\n",
            "Epoch 00121: loss did not improve from 1.11022\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.1393\n",
            "\n",
            "Epoch 00122: loss did not improve from 1.11022\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.1307\n",
            "\n",
            "Epoch 00123: loss did not improve from 1.11022\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 1.0947\n",
            "\n",
            "Epoch 00124: loss improved from 1.11022 to 1.09467, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 47s 470ms/step - loss: 1.1289\n",
            "\n",
            "Epoch 00125: loss did not improve from 1.09467\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 1.1294\n",
            "\n",
            "Epoch 00126: loss did not improve from 1.09467\n",
            "Epoch 127/200\n",
            " 13/100 [==>...........................] - ETA: 40s - loss: 1.1562"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-82acf841571d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "z6PDglInN2ZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create new stateful model with same weights and passing cell state for next prediction\n",
        "No need to pass all the data from beginning, can feed with 1 example per model.predict, saving state from previous prediction\n",
        "https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events"
      ]
    },
    {
      "metadata": {
        "id": "E-NfxdXlOSH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "e0007c9c-f1e9-4944-9b46-3fafc651af54"
      },
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(LSTM(500, batch_input_shape=(1, None, n_vocab), return_sequences=True, stateful=True))\n",
        "model2.add(LSTM(500, batch_input_shape=(1, None, n_vocab), return_sequences=True, stateful=True))\n",
        "model2.add(LSTM(500, batch_input_shape=(1, None, n_vocab), return_sequences=False, stateful=True))\n",
        "\n",
        "model2.add(Dense(n_vocab, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model2.summary()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (1, None, 500)            1112000   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (1, None, 500)            2002000   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, 500)                  2002000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, 55)                   27555     \n",
            "=================================================================\n",
            "Total params: 5,143,555\n",
            "Trainable params: 5,143,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8mj5q7TvGk6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "model2.load_weights(filename)\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q4QLKskonx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predict next letter, save state, predict next letter with saved state"
      ]
    },
    {
      "metadata": {
        "id": "gSZPwInTjcAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_temp(x, temp=1):\n",
        "  x = x**temp\n",
        "  return x/sum(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZD7FuFmZXHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "13da45ed-3c14-4d71-8782-066de319c645"
      },
      "cell_type": "code",
      "source": [
        "model2.reset_states()\n",
        "\n",
        "question = 'What is the meaning of life, death and everything?'\n",
        "question_features = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "#no need to fill array with zeros\n",
        "#question_features = np.append(np.zeros(shape=(max(seq_length-question_features.shape[0],0), n_vocab)), question_features, axis=0)\n",
        "\n",
        "print(question)\n",
        "\n",
        "#predict question char by char\n",
        "\n",
        "sys.stdout.write(' ')\n",
        "\n",
        "\n",
        "\n",
        "sys.stdout.write('\\n')\n",
        "\n",
        "for t in np.arange(0,3.1,0.5):\n",
        "  \n",
        "  model2.reset_states()\n",
        "  \n",
        "  for question_char in question_features:\n",
        "    model_response = model2.predict(question_char.reshape(1,1,n_vocab))\n",
        "  \n",
        "  print('Temperature: '+str(t))\n",
        "  \n",
        "  text_response = ''\n",
        "  \n",
        "  for _ in range(150):\n",
        "\n",
        "    model_input = np.zeros(n_vocab)\n",
        "    model_input[char_n] = 1\n",
        "    model_response = model2.predict(model_input.reshape(1,1,n_vocab))[0]\n",
        "    char_n = np.random.choice(len(model_response), p=apply_temp(model_response, temp=t))\n",
        "    print_char = int_to_char[char_n]\n",
        "    text_response += print_char\n",
        "    \n",
        "  print(text_response,'\\n')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the meaning of life, death and everything?\n",
            " \n",
            "Temperature: 0.0\n",
            "yzHUxRYmGB tcQhVuuRzGKt?WoKMQlCBuNhIdYruunmBWCoODgCDTEKCQ-JKcmeFFvNlePBWrPtMOjFQWmHcWHtMcffuQAxkGl-i-oqDJeHhQcItbnPCJgLYDYTVqkaTLEcTI,ep,bb-MTCNnNU iQ \n",
            "\n",
            "Temperature: 0.5\n",
            "es front? How can youuriezwerely arise fongun. Judo tyming aut.tWi seeias tolds he tany? Dood. Learts heme work ,iresincepons, bo,mvaib, becielts? nis \n",
            "\n",
            "Temperature: 1.0\n",
            "The more being all must be suffering bnyond truth, all. The I am man? You cannot know my attitude and nexarns to save fire, cause -- ir awarenes sappi \n",
            "\n",
            "Temperature: 1.5\n",
            "Tners is the Supreme. What is so more only as a mine. I am not still in the real is in surrender and silence and consciousness. That I am. Whatever th \n",
            "\n",
            "Temperature: 2.0\n",
            "I am consciousness and needs. Another states of the mind is easy. What is beyond the self cannot be a state of experience. You are what is the world t \n",
            "\n",
            "Temperature: 2.5\n",
            "S for the mind and pleasure and seeks for the mind. I am not that I am not that it is the same time. I am not afraid of the person and the self and th \n",
            "\n",
            "Temperature: 3.0\n",
            "I am not that the mind is so in the world. The silence is the body is all that is consciousness and the mind and the world is a person and the world i \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xao_Kxx7ZkEQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Word level model\n",
        "Using GLOVE pre-computed word embeddings http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation\n",
        "\n",
        "https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\n",
        "\n",
        "\n",
        "Glove embedding is used only for decoding data into vector space, then lstm trained as usual regression problem. For text generation embedding is used to decode model forecasts back into words (closest to forecasted vector). Temperature can be used as finding n closest words and use distance as probabilities\n"
      ]
    },
    {
      "metadata": {
        "id": "nKovml72BAbC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/My Drive/nondualbot/glove.6B.100d.txt', 'r') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFvvcjNld_vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aab3004-fc3f-42e1-c135-dad67069492d"
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "file.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "37w7Q0ZvzRke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings_len = len(embeddings_index['the'])\n",
        "embeddings_index['endofsentence'] = np.random.random(embeddings_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmIdiSRF0kim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5f815d0e-fa76-4725-97e0-47d3552ec96f"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "data_prep = data.replace('.', ' endofsentence ').lower()\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "data_prep = data_prep.translate(table)\n",
        "data_prep = data_prep.split(' ')\n",
        "data_prep = ' '.join(data_prep)\n",
        "print(data_prep[:250])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that which permeates all which nothing transcends and which like the universal space around us fills everything completely from within and without that supreme nondual brahman  that thou art endofsentence  the seeker is he who is in search of himself\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxazXyrcFuE9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_gloves = []\n",
        "new_words_counter = 0\n",
        "\n",
        "for token in data_prep.split(' '):\n",
        "  glove_vec = embeddings_index.get(token)\n",
        "  \n",
        "  if glove_vec is None:\n",
        "    glove_vec = np.random.random(embeddings_len)\n",
        "    embeddings_index[token] = glove_vec\n",
        "    new_words_counter+=1\n",
        "    #print(token)\n",
        "  \n",
        "  data_gloves.append(glove_vec)\n",
        "  \n",
        "print(new_words_counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcIHOQzu8yuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_words = len(data_gloves)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZ4DLnOaLUvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_size = 25\n",
        "\n",
        "def train_generator_words():\n",
        "  while True:\n",
        "\n",
        "    #seq_length = np.random.randint(low=2, high=200)\n",
        "    #x_train, y_train = [], []\n",
        "    \n",
        "    cur_pos = np.random.randint(low=0, high=n_words-seq_size-1)\n",
        "\n",
        "    x_train = data_gloves[cur_pos:cur_pos+seq_size]\n",
        "    y_train = data_gloves[cur_pos+1:cur_pos+seq_size+1]\n",
        "\n",
        "    yield np.reshape(x_train, (1, seq_size, embeddings_len)), np.reshape(y_train, (1, seq_size, embeddings_len))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ak7JDO1TMECQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "41006d1a-aa10-4bbd-f85f-e9178d2faf16"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "word_model = Sequential()\n",
        "word_model.add(LSTM(256, input_shape=(None, embeddings_len), return_sequences=True, stateful=False))\n",
        "#word_model.add(Dropout(0.5))\n",
        "word_model.add(LSTM(128, input_shape=(None, embeddings_len), return_sequences=True, stateful=False))\n",
        "#word_model.add(Dropout(0.5))\n",
        "\n",
        "word_model.add(Dense(embeddings_len, activation='linear'))\n",
        "word_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "word_model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, None, 256)         365568    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, None, 128)         197120    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 100)         12900     \n",
            "=================================================================\n",
            "Total params: 575,588\n",
            "Trainable params: 575,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wS13g8AeOBGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "497d21aa-9617-43b6-dc00-9fe805593987"
      },
      "cell_type": "code",
      "source": [
        "word_model.fit_generator(train_generator_words(), steps_per_epoch=100, epochs=200, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "100/100 [==============================] - 10s 103ms/step - loss: 0.2068\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.1774\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.1669\n",
            "Epoch 4/200\n",
            " 91/100 [==========================>...] - ETA: 0s - loss: 0.1716"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}