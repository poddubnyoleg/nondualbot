{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poddubnyoleg/nondualbot/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bGc0Vcfy2zQJ",
        "colab_type": "code",
        "outputId": "a2ff3a26-c347-4667-ad9b-3f5bcae04ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zxZbSUTb4lTk",
        "colab_type": "code",
        "outputId": "1cb2e430-6f3c-4aeb-bf2a-dd51b1096079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8JWl2n_n6LVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/My Drive/nondualbot/i_am_that.txt', 'r') \n",
        "data = file.read() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwJjADvv7vzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f757a283-4c5c-46e9-b70e-1bac2f6a555c"
      },
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That which permeates all, which nothing transcends and which, like the universal space around us, fi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "edNZraxrSXoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(data)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSO_sRT_Sq4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dbee74ec-b905-4b8c-eb64-7dfb1d48fd05"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_chars = len(data)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  988168\n",
            "Total Vocab:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G6xQoIj3S2M7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each chunk of data [seq_length] we take one-hot-encoded dataX array and dataY array (same but shifted +1)"
      ]
    },
    {
      "metadata": {
        "id": "qdJx7TCHUniR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ec1463bb-bea6-411d-e8f6-5e2886ebcdd9"
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, None, 512)         1163264   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 55)          28215     \n",
            "=================================================================\n",
            "Total params: 5,389,879\n",
            "Trainable params: 5,389,879\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmkaoEyLZYc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9G0Bo1CZOYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7KEkkjrFOHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_data = [char_to_int[char] for char in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW5WyuvU-gUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Inner state doesnt share between samples inside of one batch, but can share between batches, if stateful=True\n",
        "* fit(shuffle=) doesn't relate to RNNs, because to keep inner state consistent withi epoch, you need to set stateful=True and batch_size=1, so there is nothing to shuffle\n",
        "* https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ubC4zNNSEOiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "def train_generator():\n",
        "  while True:\n",
        "\n",
        "    seq_length = np.random.randint(low=2, high=200)\n",
        "    x_train, y_train = [], []\n",
        "    \n",
        "    for _ in range(batch_size):\n",
        "      \n",
        "      cur_pos = np.random.randint(low=0, high=n_chars-seq_length-1)\n",
        "\n",
        "      x_train.append(np_utils.to_categorical(num_data[cur_pos:cur_pos+seq_length], num_classes=n_vocab))\n",
        "      y_train.append(np_utils.to_categorical(num_data[cur_pos+1:cur_pos+seq_length+1], num_classes=n_vocab))\n",
        "\n",
        "    yield np.reshape(x_train, (batch_size, seq_length, n_vocab)), np.reshape(y_train, (batch_size, seq_length, n_vocab))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjtK8ogbGItq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3560
        },
        "outputId": "01a06881-bb1f-4277-eeef-bac45e71bc83"
      },
      "cell_type": "code",
      "source": [
        "h3 = model.fit_generator(train_generator(), steps_per_epoch=1000, epochs=500, verbose=1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1000/1000 [==============================] - 587s 587ms/step - loss: 2.2211\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.22107, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 2/500\n",
            "1000/1000 [==============================] - 587s 587ms/step - loss: 1.4822\n",
            "\n",
            "Epoch 00002: loss improved from 2.22107 to 1.48219, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 3/500\n",
            "1000/1000 [==============================] - 603s 603ms/step - loss: 1.3465\n",
            "\n",
            "Epoch 00003: loss improved from 1.48219 to 1.34654, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 4/500\n",
            "1000/1000 [==============================] - 581s 581ms/step - loss: 1.2981\n",
            "\n",
            "Epoch 00004: loss improved from 1.34654 to 1.29814, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 5/500\n",
            "1000/1000 [==============================] - 573s 573ms/step - loss: 1.2486\n",
            "\n",
            "Epoch 00005: loss improved from 1.29814 to 1.24861, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 6/500\n",
            "1000/1000 [==============================] - 584s 584ms/step - loss: 1.2246\n",
            "\n",
            "Epoch 00006: loss improved from 1.24861 to 1.22460, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 7/500\n",
            "1000/1000 [==============================] - 581s 581ms/step - loss: 1.1926\n",
            "\n",
            "Epoch 00007: loss improved from 1.22460 to 1.19256, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 8/500\n",
            "1000/1000 [==============================] - 578s 578ms/step - loss: 1.1840\n",
            "\n",
            "Epoch 00008: loss improved from 1.19256 to 1.18400, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 9/500\n",
            "1000/1000 [==============================] - 597s 597ms/step - loss: 1.1637\n",
            "\n",
            "Epoch 00009: loss improved from 1.18400 to 1.16372, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 10/500\n",
            "1000/1000 [==============================] - 576s 576ms/step - loss: 1.1537\n",
            "\n",
            "Epoch 00010: loss improved from 1.16372 to 1.15374, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 11/500\n",
            "1000/1000 [==============================] - 585s 585ms/step - loss: 1.1498\n",
            "\n",
            "Epoch 00011: loss improved from 1.15374 to 1.14985, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 12/500\n",
            "1000/1000 [==============================] - 589s 589ms/step - loss: 1.1313\n",
            "\n",
            "Epoch 00012: loss improved from 1.14985 to 1.13126, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 13/500\n",
            "1000/1000 [==============================] - 585s 585ms/step - loss: 1.1405\n",
            "\n",
            "Epoch 00013: loss did not improve from 1.13126\n",
            "Epoch 14/500\n",
            "1000/1000 [==============================] - 582s 582ms/step - loss: 1.1185\n",
            "\n",
            "Epoch 00014: loss improved from 1.13126 to 1.11855, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 15/500\n",
            "1000/1000 [==============================] - 581s 581ms/step - loss: 1.1195\n",
            "\n",
            "Epoch 00015: loss did not improve from 1.11855\n",
            "Epoch 16/500\n",
            "1000/1000 [==============================] - 581s 581ms/step - loss: 1.1077\n",
            "\n",
            "Epoch 00016: loss improved from 1.11855 to 1.10771, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 17/500\n",
            "1000/1000 [==============================] - 594s 594ms/step - loss: 1.0998\n",
            "\n",
            "Epoch 00017: loss improved from 1.10771 to 1.09980, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 18/500\n",
            "1000/1000 [==============================] - 595s 595ms/step - loss: 1.0854\n",
            "\n",
            "Epoch 00018: loss improved from 1.09980 to 1.08540, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 19/500\n",
            "1000/1000 [==============================] - 584s 584ms/step - loss: 1.0937\n",
            "\n",
            "Epoch 00019: loss did not improve from 1.08540\n",
            "Epoch 20/500\n",
            "1000/1000 [==============================] - 599s 599ms/step - loss: 1.0905\n",
            "\n",
            "Epoch 00020: loss did not improve from 1.08540\n",
            "Epoch 21/500\n",
            "1000/1000 [==============================] - 580s 580ms/step - loss: 1.0868\n",
            "\n",
            "Epoch 00021: loss did not improve from 1.08540\n",
            "Epoch 22/500\n",
            "1000/1000 [==============================] - 577s 577ms/step - loss: 1.0852\n",
            "\n",
            "Epoch 00022: loss improved from 1.08540 to 1.08519, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 23/500\n",
            "1000/1000 [==============================] - 583s 583ms/step - loss: 1.0784\n",
            "\n",
            "Epoch 00023: loss improved from 1.08519 to 1.07838, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 24/500\n",
            "1000/1000 [==============================] - 588s 588ms/step - loss: 1.0747\n",
            "\n",
            "Epoch 00024: loss improved from 1.07838 to 1.07468, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 25/500\n",
            "1000/1000 [==============================] - 595s 595ms/step - loss: 1.0637\n",
            "\n",
            "Epoch 00025: loss improved from 1.07468 to 1.06370, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 26/500\n",
            "1000/1000 [==============================] - 571s 571ms/step - loss: 1.0791\n",
            "\n",
            "Epoch 00026: loss did not improve from 1.06370\n",
            "Epoch 27/500\n",
            "1000/1000 [==============================] - 599s 599ms/step - loss: 1.0544\n",
            "\n",
            "Epoch 00027: loss improved from 1.06370 to 1.05443, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 28/500\n",
            "1000/1000 [==============================] - 597s 597ms/step - loss: 1.0549\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.05443\n",
            "Epoch 29/500\n",
            "1000/1000 [==============================] - 591s 591ms/step - loss: 1.0584\n",
            "\n",
            "Epoch 00029: loss did not improve from 1.05443\n",
            "Epoch 30/500\n",
            "1000/1000 [==============================] - 567s 567ms/step - loss: 1.0646\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.05443\n",
            "Epoch 31/500\n",
            "1000/1000 [==============================] - 592s 592ms/step - loss: 1.0497\n",
            "\n",
            "Epoch 00031: loss improved from 1.05443 to 1.04974, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 32/500\n",
            "1000/1000 [==============================] - 578s 578ms/step - loss: 1.0552\n",
            "\n",
            "Epoch 00032: loss did not improve from 1.04974\n",
            "Epoch 33/500\n",
            "1000/1000 [==============================] - 586s 586ms/step - loss: 1.0461\n",
            "\n",
            "Epoch 00033: loss improved from 1.04974 to 1.04612, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 34/500\n",
            "1000/1000 [==============================] - 588s 588ms/step - loss: 1.0520\n",
            "\n",
            "Epoch 00034: loss did not improve from 1.04612\n",
            "Epoch 35/500\n",
            "1000/1000 [==============================] - 595s 595ms/step - loss: 1.0383\n",
            "\n",
            "Epoch 00035: loss improved from 1.04612 to 1.03827, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 36/500\n",
            "1000/1000 [==============================] - 574s 574ms/step - loss: 1.0493\n",
            "\n",
            "Epoch 00036: loss did not improve from 1.03827\n",
            "Epoch 37/500\n",
            "1000/1000 [==============================] - 593s 593ms/step - loss: 1.0298\n",
            "\n",
            "Epoch 00037: loss improved from 1.03827 to 1.02977, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 38/500\n",
            " 820/1000 [=======================>......] - ETA: 1:44 - loss: 1.0417"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-06c2269f03f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "z6PDglInN2ZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create new stateful model with same weights and passing cell state for next prediction\n",
        "No need to pass all the data from beginning, can feed with 1 example per model.predict, saving state from previous prediction\n",
        "https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events"
      ]
    },
    {
      "metadata": {
        "id": "E-NfxdXlOSH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "forecast_model = Sequential()\n",
        "forecast_model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=True, \n",
        "               stateful=True, batch_input_shape=(1,None,n_vocab)))\n",
        "forecast_model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=True, \n",
        "               stateful=True))\n",
        "forecast_model.add(LSTM(512, input_shape=(None, n_vocab), return_sequences=False, \n",
        "               stateful=True))\n",
        "\n",
        "forecast_model.add(Dense(n_vocab, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mj5q7TvGk6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "forecast_model.load_weights(filename)\n",
        "forecast_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HtmF-2hTNkWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q4QLKskonx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predict next letter, save state, predict next letter with saved state"
      ]
    },
    {
      "metadata": {
        "id": "6ZD7FuFmZXHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0541b936-924d-419c-f173-c88f39bb8ac7"
      },
      "cell_type": "code",
      "source": [
        "forecast_model.reset_states()\n",
        "\n",
        "question = 'That which permeates all.'\n",
        "question_features = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "#question_features = np.append(np.zeros(shape=(max(seq_length-question_features.shape[0],0), n_vocab)), question_features, axis=0)\n",
        "\n",
        "print(question)\n",
        "\n",
        "#predict question char by char\n",
        "\n",
        "sys.stdout.write(' ')\n",
        "\n",
        "for question_char in question_features:\n",
        "  question_predicted_char = forecast_model.predict(question_char.reshape(1,1,n_vocab))\n",
        "  print_char = int_to_char[np.argmax(question_predicted_char)]\n",
        "  sys.stdout.write(print_char)\n",
        "  \n",
        "sys.stdout.write('\\n')\n",
        "\n",
        "for _ in range(150):\n",
        "  question_predicted_char = forecast_model.predict(question_predicted_char.reshape(1,1,n_vocab))\n",
        "  print_char = int_to_char[np.argmax(question_predicted_char)]\n",
        "  sys.stdout.write(print_char)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            " het ihich irrcaated inl  \n",
            "Th the wondle irestion of the wars of taids and tentral sort of tonds, aontiness and seadinge . Io tong ooted aont frised  are to song ond to chlled a"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xao_Kxx7ZkEQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Old prediction"
      ]
    },
    {
      "metadata": {
        "id": "JJgvz9qBTzZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76502708-0d7b-41bf-c16d-f5b6f38c1c80"
      },
      "cell_type": "code",
      "source": [
        "question = 'That which permeates all.'\n",
        "question_features = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "question_features = np.append(np.zeros(shape=(max(seq_length-question_features.shape[0],0), n_vocab)), question_features, axis=0)\n",
        "print(question)\n",
        "\n",
        "test_prediction = model.predict(question_features.reshape(1,len(question_features),n_vocab))[0]\n",
        "sys.stdout.write(' ')\n",
        "for test_char in test_prediction:\n",
        "  print_char = int_to_char[np.argmax(test_char)]\n",
        "  sys.stdout.write(print_char)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            "  ees es  aa   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa   aaaddaaaada      het ihich irrcaated all  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6GHzLmTJQuSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8c045b6a-a056-416d-caba-fd5bbe5ea3d8"
      },
      "cell_type": "code",
      "source": [
        "question = 'That which permeates all.'\n",
        "pattern = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "pattern = np.append(np.zeros(shape=(max(seq_length-pattern.shape[0],0), n_vocab)), pattern, axis=0)\n",
        "\n",
        "print(question)\n",
        "\n",
        "test_prediction = model.predict(pattern.reshape(1,len(pattern),n_vocab))[0]\n",
        "sys.stdout.write(' ')\n",
        "for test_char in test_prediction:\n",
        "  print_char = int_to_char[np.argmax(test_char)]\n",
        "  sys.stdout.write(print_char)\n",
        "\n",
        "# generate characters\n",
        "for i in range(150):\n",
        "\n",
        "  prediction = model.predict(np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1])), verbose=0)[0][-1]\n",
        "  \n",
        "  index = np.random.choice(len(prediction), p=(prediction**2)/sum(prediction**2))#np.argmax(prediction)#\n",
        "  result = int_to_char[index]\n",
        "  sys.stdout.write(result)\n",
        "  pattern = np.append(pattern, [np_utils.to_categorical(index,num_classes=n_vocab)], axis=0)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            "  ees es  aa   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa   aaaddaaaada      het ihich irrcaated all   The self is the power of life, which is not perfect. A sense of being is beyond the mind and the universe is not the subject. What can the heart of t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oVhh9O8OPJ_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f27f9b11-6a35-4e64-d9e4-4866f22c1b4e"
      },
      "cell_type": "code",
      "source": [
        "model.predict(np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1])), verbose=0)[0][-1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999940e-01, 5.4691748e-07, 4.8891195e-12, 1.0469446e-09,\n",
              "       2.6797808e-10, 6.7335869e-22, 2.8095636e-26, 6.3552258e-29,\n",
              "       7.0487436e-23, 6.3646487e-26, 3.1674805e-26, 4.9144446e-28,\n",
              "       1.0800542e-21, 1.1273043e-25, 3.6994567e-28, 6.7637692e-34,\n",
              "       3.3639891e-28, 2.0014950e-27, 2.9739306e-31, 5.9701945e-25,\n",
              "       2.0236516e-27, 0.0000000e+00, 1.8087869e-34, 1.6505187e-24,\n",
              "       1.3041807e-22, 2.8014332e-31, 8.8703745e-35, 1.8485967e-19,\n",
              "       6.2777927e-26, 6.5178172e-19, 1.2295411e-16, 1.3757429e-19,\n",
              "       6.6819938e-13, 3.0502608e-13, 2.4411573e-12, 2.3483241e-15,\n",
              "       1.7099024e-17, 1.6124142e-16, 2.0943558e-22, 2.6491773e-17,\n",
              "       1.4324105e-12, 8.1101988e-14, 7.6933120e-11, 5.4370323e-14,\n",
              "       4.9228969e-17, 4.9811773e-23, 2.1248222e-15, 8.4897835e-12,\n",
              "       2.5380966e-16, 1.9712886e-15, 8.8841031e-17, 3.3209028e-15,\n",
              "       2.6880777e-22, 6.2216196e-20, 2.2106736e-28], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "lChG5UQGlewY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "70e55f41-d38d-42c9-faa8-c3b052ac648c"
      },
      "cell_type": "code",
      "source": [
        "np.append(pattern, [np_utils.to_categorical(index,num_classes=n_vocab)], axis=0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "4ghdt5Z3mhhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3904acd3-afc1-4e70-d68d-0818c7792194"
      },
      "cell_type": "code",
      "source": [
        "patte"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "DfwWaffxnHuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7eb2936-9a7f-4a50-cd88-e44d5008a656"
      },
      "cell_type": "code",
      "source": [
        "np.argmax(dataY[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "40kQZZqnA_T8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "11c81b77-8bf1-4ef8-def8-3c2c59fd51ab"
      },
      "cell_type": "code",
      "source": [
        "dataY[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "nKovml72BAbC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}