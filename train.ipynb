{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poddubnyoleg/nondualbot/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eGFSolPe6pD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events\n",
        "\n",
        "https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "bGc0Vcfy2zQJ",
        "colab_type": "code",
        "outputId": "c130bd3a-50e5-4481-bcfb-8d7006fe33ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zxZbSUTb4lTk",
        "colab_type": "code",
        "outputId": "27d85cf4-80f1-4d27-9b33-f34d08e78032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8JWl2n_n6LVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/My Drive/nondualbot/i_am_that.txt', 'r') \n",
        "data = file.read() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwJjADvv7vzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50be9598-b4e4-4302-bf46-7bd982694ef8"
      },
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That which permeates all, which nothing transcends and which, like the universal space around us, fi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "edNZraxrSXoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(data)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSO_sRT_Sq4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3962c27d-6130-44e3-d216-5b34d26175df"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_chars = len(data)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  988168\n",
            "Total Vocab:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G6xQoIj3S2M7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each chunk of data [seq_length] we take one-hot-encoded dataX array and dataY array (same but shifted +1)"
      ]
    },
    {
      "metadata": {
        "id": "qdJx7TCHUniR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6775a28b-733d-4cec-e9bc-dfec6d316b60"
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(500, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(250, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(250, input_shape=(None, n_vocab), return_sequences=True, stateful=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, None, 500)         1112000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 500)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 250)         751000    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 250)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 250)         501000    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 250)         0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 55)          13805     \n",
            "=================================================================\n",
            "Total params: 2,377,805\n",
            "Trainable params: 2,377,805\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmkaoEyLZYc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9G0Bo1CZOYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7KEkkjrFOHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_data = [char_to_int[char] for char in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW5WyuvU-gUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**train with batch generator with fixed or variable batch sequense size**\n",
        "* Inner state doesnt share between samples inside of one batch, but can share between batches, if stateful=True\n",
        "* fit(shuffle=) doesn't relate to RNNs, because to keep inner state consistent withi epoch, you need to set stateful=True and batch_size=1, so there is nothing to shuffle\n",
        "* https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ubC4zNNSEOiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "def train_generator():\n",
        "  while True:\n",
        "\n",
        "    #seq_length = np.random.randint(low=2, high=200)\n",
        "    #x_train, y_train = [], []\n",
        "    \n",
        "    cur_pos = np.random.randint(low=0, high=n_chars-batch_size-1)\n",
        "\n",
        "    x_train = np_utils.to_categorical(num_data[cur_pos:cur_pos+batch_size], num_classes=n_vocab)\n",
        "    y_train = np_utils.to_categorical(num_data[cur_pos+1:cur_pos+batch_size+1], num_classes=n_vocab)\n",
        "\n",
        "    yield np.reshape(x_train, (1, batch_size, n_vocab)), np.reshape(y_train, (1, batch_size, n_vocab))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjtK8ogbGItq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "037d70bb-d279-4913-e819-188818c29cfe"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator(), steps_per_epoch=100, epochs=100, verbose=1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 51s 513ms/step - loss: 3.1208\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.12076, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 49s 493ms/step - loss: 3.0235\n",
            "\n",
            "Epoch 00002: loss improved from 3.12076 to 3.02352, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 49s 494ms/step - loss: 2.9900\n",
            "\n",
            "Epoch 00003: loss improved from 3.02352 to 2.98999, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 49s 492ms/step - loss: 2.7599\n",
            "\n",
            "Epoch 00004: loss improved from 2.98999 to 2.75994, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 49s 491ms/step - loss: 2.6700\n",
            "\n",
            "Epoch 00005: loss improved from 2.75994 to 2.67005, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 49s 494ms/step - loss: 2.5042\n",
            "\n",
            "Epoch 00006: loss improved from 2.67005 to 2.50425, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 49s 492ms/step - loss: 2.4015\n",
            "\n",
            "Epoch 00007: loss improved from 2.50425 to 2.40155, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 49s 489ms/step - loss: 2.3245\n",
            "\n",
            "Epoch 00008: loss improved from 2.40155 to 2.32455, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 2.2343\n",
            "\n",
            "Epoch 00009: loss improved from 2.32455 to 2.23428, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 2.1857\n",
            "\n",
            "Epoch 00010: loss improved from 2.23428 to 2.18569, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 2.1436\n",
            "\n",
            "Epoch 00011: loss improved from 2.18569 to 2.14359, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 49s 492ms/step - loss: 2.1323\n",
            "\n",
            "Epoch 00012: loss improved from 2.14359 to 2.13230, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 49s 491ms/step - loss: 2.0497\n",
            "\n",
            "Epoch 00013: loss improved from 2.13230 to 2.04972, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 49s 491ms/step - loss: 2.0131\n",
            "\n",
            "Epoch 00014: loss improved from 2.04972 to 2.01312, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 49s 491ms/step - loss: 1.9979\n",
            "\n",
            "Epoch 00015: loss improved from 2.01312 to 1.99794, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 1.9678\n",
            "\n",
            "Epoch 00016: loss improved from 1.99794 to 1.96783, saving model to /content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 1.9778\n",
            "\n",
            "Epoch 00017: loss did not improve from 1.96783\n",
            "Epoch 18/100\n",
            " 52/100 [==============>...............] - ETA: 23s - loss: 1.8857"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z6PDglInN2ZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create new stateful model with same weights and passing cell state for next prediction\n",
        "No need to pass all the data from beginning, can feed with 1 example per model.predict, saving state from previous prediction\n",
        "https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events"
      ]
    },
    {
      "metadata": {
        "id": "E-NfxdXlOSH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "11b20adb-dc91-4d03-e2bd-59bd55cdc177"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(250, batch_input_shape=(1, None, n_vocab), return_sequences=True, stateful=True))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(125, batch_input_shape=(1, None, n_vocab), return_sequences=False, stateful=True))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Dense(n_vocab, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model2.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (1, None, 250)            306000    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (1, None, 250)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (1, 125)                  188000    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (1, 125)                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, 55)                   6930      \n",
            "=================================================================\n",
            "Total params: 500,930\n",
            "Trainable params: 500,930\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8mj5q7TvGk6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/My Drive/nondualbot/best_current_weights_2.hdf5\"\n",
        "model2.load_weights(filename)\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q4QLKskonx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predict next letter, save state, predict next letter with saved state"
      ]
    },
    {
      "metadata": {
        "id": "6ZD7FuFmZXHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1981e312-b9f8-4fad-bdbf-573bc0749b84"
      },
      "cell_type": "code",
      "source": [
        "model2.reset_states()\n",
        "\n",
        "question = 'That which permeates all.'\n",
        "question_features = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "#no need to fill array with zeros\n",
        "#question_features = np.append(np.zeros(shape=(max(seq_length-question_features.shape[0],0), n_vocab)), question_features, axis=0)\n",
        "\n",
        "print(question)\n",
        "\n",
        "#predict question char by char\n",
        "\n",
        "sys.stdout.write(' ')\n",
        "\n",
        "for question_char in question_features:\n",
        "  model_response = model2.predict(question_char.reshape(1,1,n_vocab))\n",
        "  char_n = np.argmax(model_response)\n",
        "  print_char = int_to_char[char_n]\n",
        "  sys.stdout.write(print_char)\n",
        "\n",
        "sys.stdout.write('\\n')\n",
        "\n",
        "for _ in range(150):\n",
        "  model_input = np.zeros(n_vocab)\n",
        "  model_input[char_n] = 1\n",
        "  model_response = model2.predict(model_input.reshape(1,1,n_vocab))[0]\n",
        "  char_n = np.random.choice(len(model_response), p=(model_response**2)/sum(model_response**2))#np.argmax(model_response)#\n",
        "  print_char = int_to_char[char_n]\n",
        "  sys.stdout.write(print_char)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            " het ihach iersenne  anl  \n",
            "I am is self and mate is the present to the body is the porson is the knower is the juttent is with will undinting and the mind is a with and a cannot"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xao_Kxx7ZkEQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Old prediction"
      ]
    },
    {
      "metadata": {
        "id": "JJgvz9qBTzZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81065306-60cf-464a-997a-d55d67c6f791"
      },
      "cell_type": "code",
      "source": [
        "question = 'That which permeates all.'\n",
        "question_features = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "question_features = np.append(np.zeros(shape=(max(seq_length-question_features.shape[0],0), n_vocab)), question_features, axis=0)\n",
        "print(question)\n",
        "\n",
        "test_prediction = model.predict(question_features.reshape(1,len(question_features),n_vocab))[0]\n",
        "sys.stdout.write(' ')\n",
        "for test_char in test_prediction:\n",
        "  print_char = int_to_char[np.argmax(test_char)]\n",
        "  sys.stdout.write(print_char)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            "   eeis                                                                      eniahach iersenne  anl  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6GHzLmTJQuSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4feda04f-c035-4c90-c4d4-77c38136c9bd"
      },
      "cell_type": "code",
      "source": [
        "question = 'That which permeates all.'\n",
        "pattern = np_utils.to_categorical([char_to_int[char] for char in question], num_classes=n_vocab)\n",
        "pattern = np.append(np.zeros(shape=(max(seq_length-pattern.shape[0],0), n_vocab)), pattern, axis=0)\n",
        "\n",
        "print(question)\n",
        "\n",
        "test_prediction = model.predict(pattern.reshape(1,len(pattern),n_vocab))[0]\n",
        "sys.stdout.write(' ')\n",
        "for test_char in test_prediction:\n",
        "  print_char = int_to_char[np.argmax(test_char)]\n",
        "  sys.stdout.write(print_char)\n",
        "\n",
        "sys.stdout.write('\\n')\n",
        "# generate characters\n",
        "for i in range(150):\n",
        "\n",
        "  prediction = model.predict(np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1])), verbose=0)[0][-1]\n",
        "  \n",
        "  index = np.argmax(prediction)#np.random.choice(len(prediction), p=(prediction**2)/sum(prediction**2))#\n",
        "  result = int_to_char[index]\n",
        "  sys.stdout.write(result)\n",
        "  pattern = np.append(pattern, [np_utils.to_categorical(index,num_classes=n_vocab)], axis=0)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That which permeates all.\n",
            "   eeis                                                                      eniahach iersenne  anl  \n",
            " I am the world and the see the world and the see the world and the see the world and the see the world and the see the world and the see the world an"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oVhh9O8OPJ_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f27f9b11-6a35-4e64-d9e4-4866f22c1b4e"
      },
      "cell_type": "code",
      "source": [
        "model.predict(np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1])), verbose=0)[0][-1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999940e-01, 5.4691748e-07, 4.8891195e-12, 1.0469446e-09,\n",
              "       2.6797808e-10, 6.7335869e-22, 2.8095636e-26, 6.3552258e-29,\n",
              "       7.0487436e-23, 6.3646487e-26, 3.1674805e-26, 4.9144446e-28,\n",
              "       1.0800542e-21, 1.1273043e-25, 3.6994567e-28, 6.7637692e-34,\n",
              "       3.3639891e-28, 2.0014950e-27, 2.9739306e-31, 5.9701945e-25,\n",
              "       2.0236516e-27, 0.0000000e+00, 1.8087869e-34, 1.6505187e-24,\n",
              "       1.3041807e-22, 2.8014332e-31, 8.8703745e-35, 1.8485967e-19,\n",
              "       6.2777927e-26, 6.5178172e-19, 1.2295411e-16, 1.3757429e-19,\n",
              "       6.6819938e-13, 3.0502608e-13, 2.4411573e-12, 2.3483241e-15,\n",
              "       1.7099024e-17, 1.6124142e-16, 2.0943558e-22, 2.6491773e-17,\n",
              "       1.4324105e-12, 8.1101988e-14, 7.6933120e-11, 5.4370323e-14,\n",
              "       4.9228969e-17, 4.9811773e-23, 2.1248222e-15, 8.4897835e-12,\n",
              "       2.5380966e-16, 1.9712886e-15, 8.8841031e-17, 3.3209028e-15,\n",
              "       2.6880777e-22, 6.2216196e-20, 2.2106736e-28], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "lChG5UQGlewY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "70e55f41-d38d-42c9-faa8-c3b052ac648c"
      },
      "cell_type": "code",
      "source": [
        "np.append(pattern, [np_utils.to_categorical(index,num_classes=n_vocab)], axis=0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "4ghdt5Z3mhhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3904acd3-afc1-4e70-d68d-0818c7792194"
      },
      "cell_type": "code",
      "source": [
        "patte"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "DfwWaffxnHuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7eb2936-9a7f-4a50-cd88-e44d5008a656"
      },
      "cell_type": "code",
      "source": [
        "np.argmax(dataY[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "40kQZZqnA_T8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "11c81b77-8bf1-4ef8-def8-3c2c59fd51ab"
      },
      "cell_type": "code",
      "source": [
        "dataY[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "nKovml72BAbC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}